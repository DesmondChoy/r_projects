---
title: Hip-Hop Lyrics Be Flowing with Tidy Text Mining
author: Desmond Choy
date: '2020-05-16'
slug: hip-hop-lyrics-be-flowin-with-tidy-text-mining
summary: Hip-hop songs' sentiment is extracted and visualized. Regularized regression is then trained on lyrics to predict if a hip-hop song was old school (released before the year 2000) or modern.
readingtime: '10'
tags:
  - EDA
  - machine learning
  - r
  - visualization
  - NLP
coverImage: https://images.unsplash.com/photo-1508973379184-7517410fb0bc?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1050&q=80
thumbnailImage: https://images.unsplash.com/photo-1589929168117-cd9ec5f27ab7?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=675&q=80
thumbnailImagePosition: left
coverCaption: Hip hop By Joel Muniz
---



<p>I’ve been hit with a double dose of NLP inspiration after reading a recent Julia Silge <a href="https://juliasilge.com/blog/animal-crossing/">blog post</a> and watching a <a href="https://learn.datacamp.com/courses/sentiment-analysis-in-r-the-tidy-way">Datacamp video</a> on Tidy Text Mining which she used to teach. Ergo, this post will set out to do two things: use sentiment analysis to explore hip-hop songs, and subsequently use the tidymodels package on the lyrics to predict which era (Old School versus Modern) the songs came from.</p>
<p>The new <a href="https://www.tidymodels.org">tidymodels website</a> was launched recently and contains a wealth of information - explaining in detail the main functions of each package and how the ecosystem operates. Prior to the launch of the website, I was already getting my feet wet in a <a href="https://desmondchoy.netlify.app/2020/04/food-consumption-and-co2-emissions/">previous blog post</a> where I did some predictive modelling with tidymodels.</p>
<p>Here I intend to explore the ecosystem further and utilise <code>workflows</code> and <code>tune</code> package. The former has functions to bundle your pre-processing, modeling, and post-processing together, while the latter is meant for optimization of model’s hyperparameters as well as pre-processing steps.</p>
<pre class="r"><code>library(tidyverse)
theme_set(theme_minimal())</code></pre>
<div id="data-set" class="section level2">
<h2>Data Set</h2>
<p>This dataset comes once again from <a href="https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-04-14/readme.md">a recent TidyTuesday</a>. BBC Music asked more than 100 critics, artists, and other music industry folks from 15 countries for their five favorite hip-hop tracks and this data contains said polling results. To be clear, these are not the final results of the poll but an aggregation of their top five submissions.</p>
<pre class="r"><code>polls &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-04-14/polls.csv&#39;)
polls</code></pre>
<pre><code>## # A tibble: 535 x 9
##     rank title artist gender  year critic_name critic_rols critic_country
##    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;         
##  1     1 Term~ Publi~ male    1998 Joseph Aba~ Fat Beats   US            
##  2     2 4th ~ Gza f~ male    1995 Joseph Aba~ Fat Beats   US            
##  3     3 Pete~ Run D~ male    1986 Joseph Aba~ Fat Beats   US            
##  4     4 Play~ GLOBE~ male    2001 Joseph Aba~ Fat Beats   US            
##  5     5 Time~ O.C.   male    1994 Joseph Aba~ Fat Beats   US            
##  6     1 Play~ Slum ~ male    1997 Biba Adams  Critic      US            
##  7     2 Self~ Stop ~ mixed   1989 Biba Adams  Critic      US            
##  8     3 Push~ Salt-~ female  1986 Biba Adams  Critic      US            
##  9     4 Ambi~ 2Pac   male    1996 Biba Adams  Critic      US            
## 10     5 Big ~ JAY-Z~ male    1999 Biba Adams  Critic      US            
## # ... with 525 more rows, and 1 more variable: critic_country2 &lt;chr&gt;</code></pre>
</div>
<div id="data-cleaning" class="section level2">
<h2>Data Cleaning</h2>
<p>Let’s remove the duplicate entries, remove columns I won’t be using, and create our two buckets - “Old School” for hip-hop songs released before the year 2000, and “Modern” for songs released after 2000.</p>
<pre class="r"><code>sorted &lt;- polls %&gt;%
  distinct(title, .keep_all = TRUE) %&gt;%
  mutate(bucket = case_when(year &lt; 2000 ~ &quot;Old School&quot;,
                            TRUE ~ &quot;Modern&quot;)) %&gt;%
  select(-contains(&quot;critic&quot;), -rank)

sorted</code></pre>
<pre><code>## # A tibble: 309 x 5
##    title                   artist                          gender  year bucket  
##    &lt;chr&gt;                   &lt;chr&gt;                           &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;   
##  1 Terminator X To The Ed~ Public Enemy                    male    1998 Old Sch~
##  2 4th Chamber             Gza ft. Ghostface Killah &amp; Kil~ male    1995 Old Sch~
##  3 Peter Piper             Run DMC                         male    1986 Old Sch~
##  4 Play That Beat Mr DJ    GLOBE &amp; Whiz Kid                male    2001 Modern  
##  5 Time’s Up               O.C.                            male    1994 Old Sch~
##  6 Players                 Slum Village                    male    1997 Old Sch~
##  7 Self Destruction        Stop The Violence Movement      mixed   1989 Old Sch~
##  8 Push It                 Salt-N-Pepa                     female  1986 Old Sch~
##  9 Ambitionz Az A Ridah    2Pac                            male    1996 Old Sch~
## 10 Big Pimpin&#39;             JAY-Z ft. UGK                   male    1999 Old Sch~
## # ... with 299 more rows</code></pre>
<p>Our dataset now contains a total of 309 unique songs with a fairly even mix across Modern and Old School. I kept the <code>gender</code> column because it could yield some predictive power.</p>
<pre class="r"><code>sorted %&gt;% 
  distinct(title, .keep_all = TRUE) %&gt;% 
  count(bucket, gender)</code></pre>
<pre><code>## # A tibble: 6 x 3
##   bucket     gender     n
##   &lt;chr&gt;      &lt;chr&gt;  &lt;int&gt;
## 1 Modern     female    12
## 2 Modern     male     119
## 3 Modern     mixed      7
## 4 Old School female    11
## 5 Old School male     148
## 6 Old School mixed     12</code></pre>
</div>
<div id="obtaining-lyrics" class="section level2">
<h2>Obtaining Lyrics</h2>
<p>Using the <code>genius</code> package from <a href="https://github.com/josiahparry/genius">Josiah Parry</a>, the <code>add_genius</code> function is able to pull lyrics for song/album by specifying the artist and the title. Note: Pulling lyrics for 309 songs took slightly over 12 minutes and not all lyrics were successfully retrieved. You can choose to use the code I hash-tagged out, or the resultign dataframe that I uploaded to my GitHub.</p>
<pre class="r"><code>library(genius)

#lyrics &lt;- sorted %&gt;% add_genius(artist, title, type = &quot;lyrics&quot;)

lyrics &lt;- read_csv(&quot;https://raw.githubusercontent.com/DesmondChoy/DataSets/master/lyrics.csv&quot;)</code></pre>
<p>From an initial size of 309 songs, <code>add_genius</code> only managed to return lyrics for 142 songs. Let’s also remove NA values.</p>
<pre class="r"><code>lyrics %&gt;%
  distinct(title, .keep_all = TRUE)</code></pre>
<pre><code>## # A tibble: 142 x 8
##    title      artist   gender  year bucket track_title     line lyric           
##    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;           
##  1 Terminato~ Public ~ male    1998 Old S~ Terminator X ~     1 &quot;\&quot;At the count~
##  2 Peter Pip~ Run DMC  male    1986 Old S~ Peter Piper        1 &quot;Now Peter Pipe~
##  3 Play That~ GLOBE &amp;~ male    2001 Modern Play That Bea~     1 &quot;Punk-rock, new~
##  4 Players    Slum Vi~ male    1997 Old S~ Players            1 &quot;(Players)&quot;     
##  5 Ambitionz~ 2Pac     male    1996 Old S~ Ambitionz Az ~     1 &quot;I won&#39;t deny i~
##  6 Sucker MCs Run DMC  male    1984 Old S~ Sucker M.C.’s      1 &quot;Two years ago,~
##  7 Lyrics Of~ Eric B ~ male    1988 Old S~ Lyrics of Fury     1 &quot;I&#39;m rated \&quot;R\~
##  8 In Da Club 50 Cent  male    2003 Modern In Da Club         1  &lt;NA&gt;           
##  9 Dear Mama  2Pac     male    1995 Old S~ Dear Mama          1 &quot;You are apprec~
## 10 Fuck Tha ~ NWA      male    1988 Old S~ Fuck tha Poli~     1  &lt;NA&gt;           
## # ... with 132 more rows</code></pre>
<pre class="r"><code>lyrics &lt;- lyrics %&gt;% 
  na.omit()</code></pre>
</div>
<div id="text-mining-using-tidy-data-principles" class="section level2">
<h2>Text Mining using Tidy Data Principles</h2>
<p>The process of <a href="https://www.tidytextmining.com/index.html">tidy text mining</a> is as such:</p>
<ul>
<li>To make the lyrics dataset tidy, we need to restructure it in the <strong>one-token-per-row</strong> format using the <code>unnest_tokens()</code> function<br />
</li>
<li>Next, remove Stop Words - words that are not useful for an analysis, typically extremely common words such as <em>the</em>, <em>of</em>, <em>to</em>, and so forth in English. The <code>stop_words</code> dataset in the tidytext package contains stop words from three lexicons.<br />
</li>
<li>Removal of stop words is done using <code>anti-join()</code><br />
</li>
<li>Finally, merge the dataframe with a lexicon using <code>inner_join()</code></li>
</ul>
<p>Here, the NRC lexicon by <a href="https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm">Saif Mohammad</a> is used. The NRC lexicon categorizes words in a binary fashion (<em>yes</em>, <em>no</em>) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.</p>
<pre class="r"><code>library(tidytext)
library(textdata)

tidy_lyrics &lt;- lyrics %&gt;% 
  unnest_tokens(word, lyric) %&gt;% 
  anti_join(stop_words, by = &quot;word&quot;) %&gt;% 
  inner_join(get_sentiments(&quot;nrc&quot;)) %&gt;% 
  select(-track_title) #repeated column

tidy_lyrics %&gt;% 
  select(word, sentiment, line, title, artist, bucket) %&gt;% 
  sample_n(10)</code></pre>
<pre><code>## # A tibble: 10 x 6
##    word       sentiment     line title                artist         bucket    
##    &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;                &lt;chr&gt;          &lt;chr&gt;     
##  1 money      trust           72 Takkies              YoungstaCPT    Modern    
##  2 devil      disgust         51 One Day              UGK            Old School
##  3 faith      trust          158 Mortal Man           Kendrick Lamar Modern    
##  4 jealousy   sadness         68 Ms Jackson           OutKast        Modern    
##  5 resentment sadness         98 Mortal Man           Kendrick Lamar Modern    
##  6 roulette   anticipation    69 Bring Da Ruckus      Wu-Tang Clan   Old School
##  7 child      anticipation    50 Doo Wop (That Thing) Lauryn Hill    Old School
##  8 luck       joy             65 Wot Do U Call It?    Wiley          Modern    
##  9 crazy      fear             1 Insane In The Brain  Cypress Hill   Old School
## 10 expire     negative        29 Ha                   Juvenile       Modern</code></pre>
<p>Let’s peek at the top words used in our songs.</p>
<pre class="r"><code>tidy_lyrics %&gt;% 
  count(word, sort = TRUE) </code></pre>
<pre><code>## # A tibble: 1,686 x 2
##    word        n
##    &lt;chr&gt;   &lt;int&gt;
##  1 bitch     945
##  2 shit      840
##  3 god       650
##  4 money     522
##  5 love      420
##  6 feeling   380
##  7 bout      330
##  8 real      290
##  9 baby      272
## 10 crazy     260
## # ... with 1,676 more rows</code></pre>
<p>To get a sense of which are the primary emotions in each songs, let’s calculate the total number of words in each song, and from there calculate the percentage of each emotion using the total number of words.</p>
<pre class="r"><code>#Calculating total words per song
totals &lt;- tidy_lyrics %&gt;% 
  count(title) %&gt;% 
  rename(total_words = n)

#Joining it back to the original dataframe
lyrics_count &lt;- tidy_lyrics %&gt;% 
  left_join(totals, by = &quot;title&quot;)</code></pre>
<p>Which songs have the highest proportion of negative words?</p>
<pre class="r"><code>lyrics_count %&gt;% 
  filter(sentiment == &quot;negative&quot;) %&gt;% 
  count(title, sentiment, total_words) %&gt;% 
  ungroup() %&gt;% 
  mutate(percent = n/total_words) %&gt;% 
  arrange(desc(percent))</code></pre>
<pre><code>## # A tibble: 141 x 5
##    title                                 sentiment total_words     n percent
##    &lt;chr&gt;                                 &lt;chr&gt;           &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;
##  1 Bout It Bout It                       negative          389   150   0.386
##  2 Players                               negative           26    10   0.385
##  3 I Got 5 On It                         negative          113    41   0.363
##  4 They Want EFX                         negative          136    49   0.360
##  5 We Got It For Cheap (Intro)           negative          162    56   0.346
##  6 Mask Off                              negative          174    58   0.333
##  7 Fight Da Faida                        negative          175    58   0.331
##  8 Insane In The Brain                   negative          180    53   0.294
##  9 Straight Outta Compton (Extended Mix) negative          176    51   0.290
## 10 Get The Bozack                        negative          163    46   0.282
## # ... with 131 more rows</code></pre>
<p>Let’s visualise it across some of the other emotions provided by NRC lexicon.</p>
<pre class="r"><code>lyrics_count %&gt;%
  filter(
    sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;, &quot;joy&quot;, &quot;fear&quot;, &quot;anger&quot;, &quot;trust&quot;),
    total_words &gt; 10
  ) %&gt;%
  count(title, sentiment, total_words, artist) %&gt;%
  mutate(percent = n / total_words,
         title = paste(artist, title, sep = &quot; - &quot;)) %&gt;%
  arrange(desc(percent)) %&gt;%
  group_by(sentiment) %&gt;%
  slice(1:10) %&gt;%
  ggplot(aes(reorder_within(title, percent, sentiment), percent, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(
    . ~ fct_relevel(sentiment, &quot;positive&quot;, &quot;negative&quot;, &quot;joy&quot;, &quot;fear&quot;, &quot;trust&quot;),
    scales = &quot;free&quot;,
    ncol = 2
  ) +
  coord_flip() +
  scale_x_reordered() +
  scale_y_continuous(labels = scales::label_percent(accuracy = 1)) +
  labs(
    x = &quot;&quot;,
    y = &quot;Percentage of song&#39;s lyrics in each category&quot;,
    title = &quot;Tidy Text Mining: Using Sentiment Analysis to Group Hip-Hop Songs across Emotions&quot;,
    subtitle = &quot;Selected Hip-Hop Songs from BBC Poll; Categories from NRC Emotion Lexicon; Lyrics from Genius package&quot;
  ) +
  theme(
    plot.title = element_text(face = &quot;bold&quot;, size = 20),
    plot.title.position = &quot;plot&quot;,
    plot.subtitle = element_text(size = 15),
    strip.background = element_blank(),
    strip.text = element_text(face = &quot;bold&quot;, size = 15)
  )</code></pre>
<p><img src="/post/2020-05-16-hip-hop-lyrics-be-flowin-with-tidy-text-mining_files/figure-html/unnamed-chunk-11-1.png" width="1152" /></p>
<p>Data is wrangled further with <code>count()</code> and <code>pivot_wider()</code> to obtain a sentiment score. With the score, we can now visualise how sentiment (positive/negative) changes across ten randomly sampled rap songs.</p>
<pre class="r"><code>song_sentiment &lt;- tidy_lyrics %&gt;%
  count(title, artist, index = line %/% 4, sentiment) %&gt;%
  pivot_wider(
    names_from = sentiment,
    values_from = n,
    values_fill = list(n = 0)) %&gt;% 
  mutate(sentiment = positive - negative)

set.seed(2000)
sample &lt;- song_sentiment %&gt;% 
  distinct(title) %&gt;% 
  sample_n(10) %&gt;% 
  pull(title)
  
song_sentiment %&gt;%
  filter(title %in% sample) %&gt;%
  mutate(title = paste(artist, title, sep = &quot; - &quot;)) %&gt;%
  ggplot(aes(index, sentiment, fill = title)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(. ~ title, ncol = 2, scales = c(&quot;free&quot;)) +
  labs(
    x = &quot;Duration&quot;,
    y = &quot;Sentiment Score&quot;,
    title = &quot;Visualising Sentiment Scores for 10 randomly selected hip-hop songs&quot;,
    subtitle = &quot;Positive scores indicate positive sentiment, and vice versa.&quot;
  ) +
  theme(
    plot.title = element_text(face = &quot;bold&quot;, size = 20),
    plot.title.position = &quot;plot&quot;,
    plot.subtitle = element_text(size = 15),
    strip.background = element_blank(),
    strip.text = element_text(face = &quot;bold&quot;, size = 15)
  )</code></pre>
<p><img src="/post/2020-05-16-hip-hop-lyrics-be-flowin-with-tidy-text-mining_files/figure-html/unnamed-chunk-12-1.png" width="1152" /></p>
</div>
<div id="modelling" class="section level2">
<h2>Modelling</h2>
<p>We begin our modelling process that aims to predict if a hip-hop song is Old School (released before the year 2000) or Modern (released in the year 2000 or later) by using feature engineering on its lyrics as well as artist gender.</p>
</div>
<div id="data-splitting" class="section level2">
<h2>Data Splitting</h2>
<pre class="r"><code>library(tidymodels)

set.seed(2000)
hiphop_split &lt;- initial_split(lyrics, strata = bucket)
hiphop_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;8795/2930/11725&gt;</code></pre>
<pre class="r"><code>hiphop_train &lt;- training(hiphop_split)
hiphop_test &lt;- testing(hiphop_split)</code></pre>
</div>
<div id="resampling" class="section level2">
<h2>Resampling</h2>
<p>Since we plan on doing some hyperparameter tuning, we will need resampled data.</p>
<pre class="r"><code>set.seed(2000)
folds &lt;- vfold_cv(hiphop_train, v = 10)
folds</code></pre>
<pre><code>## #  10-fold cross-validation 
## # A tibble: 10 x 2
##    splits             id    
##    &lt;list&gt;             &lt;chr&gt; 
##  1 &lt;split [7.9K/880]&gt; Fold01
##  2 &lt;split [7.9K/880]&gt; Fold02
##  3 &lt;split [7.9K/880]&gt; Fold03
##  4 &lt;split [7.9K/880]&gt; Fold04
##  5 &lt;split [7.9K/880]&gt; Fold05
##  6 &lt;split [7.9K/879]&gt; Fold06
##  7 &lt;split [7.9K/879]&gt; Fold07
##  8 &lt;split [7.9K/879]&gt; Fold08
##  9 &lt;split [7.9K/879]&gt; Fold09
## 10 &lt;split [7.9K/879]&gt; Fold10</code></pre>
</div>
<div id="model-specification" class="section level2">
<h2>Model Specification</h2>
<p>Let’s use a <a href="https://bradleyboehmke.github.io/HOML/regularized-regression.html">LASSO</a> (least absolute shrinkage and selection operator) regression model, which has two tuning hyperparameters:<br />
* <strong>Penalty</strong>: The total amount of regularization in the model. This is set to <code>tune()</code>, a placeholder that will be determined subsequently by tuning.<br />
* <strong>Mixture</strong>: The proportion of L1 regularization in the model. Setting Mixture to 1 makes it a LASSO model.</p>
<pre class="r"><code>lr_model &lt;- logistic_reg(penalty = tune(),
                         mixture = 1) %&gt;% 
  set_engine(&quot;glmnet&quot;)</code></pre>
</div>
<div id="feature-pre-processing" class="section level2">
<h2>Feature Pre-processing</h2>
<p>Here we begin to do some processing on the lyrics that we previously pulled:</p>
<ul>
<li>step_tokenize: Similar to <code>unnest_tokens()</code>; this tokenizes our lyrics. The default option is used where words are broken down into single words (as opposed to n-grams or multiple words)</li>
<li>step_stopwords: Similar to <code>anti-join(stop_words)</code></li>
<li>step_tokenfilter: We keep the top 2000 most-used tokens after accounting for the removal of stop words</li>
<li>step_tfidf: The statistic <a href="https://www.tidytextmining.com/tfidf.html">tf-idf</a>, or term frequency multiplied by inverse document frequency, identifies words that are important to one document within a collection of documents.</li>
<li>step_normalize: Once the words have been converted to tf-idf values, they will then be centered and scaled i.e. normalized. This is important as the LASSO model is not robust to outliers.</li>
<li>step_dummy: Standard practise to convert factor/character columns (such as our <code>gender</code> bucket) to <a href="https://bookdown.org/max/FES/creating-dummy-variables-for-unordered-categories.html">dummy variables</a> to make them numeric.</li>
</ul>
<pre class="r"><code>library(textrecipes)

hiphop_rec &lt;- hiphop_train %&gt;%
  recipe(bucket ~ lyric + gender) %&gt;%
  step_tokenize(lyric) %&gt;%
  step_stopwords(lyric) %&gt;%
  step_tokenfilter(lyric, max_tokens = 2000) %&gt;%
  step_tfidf(lyric) %&gt;%
  step_normalize(all_predictors(),-gender) %&gt;%
  step_dummy(all_nominal(), -all_outcomes())

prep(hiphop_rec) </code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          2
## 
## Training data contained 8795 data points and no missing data.
## 
## Operations:
## 
## Tokenization for lyric [trained]
## Stop word removal for lyric [trained]
## Text filtering for lyric [trained]
## Term frequency-inverse document frequency with lyric [trained]
## Centering and scaling for tfidf_lyric_&lt;U+4E86&gt;, tfidf_lyric_&lt;U+4F9D&gt;&lt;U+7136&gt;, ... [trained]
## Dummy variables from gender [trained]</code></pre>
</div>
<div id="workflows" class="section level2">
<h2>Workflows</h2>
<p>We next use the <code>workflows</code> package to bundle our model and recipes. Personally I’m rather excited after this package as it has not only has great synergy with the ecosystem, but it makes the process cleaner and more intuitive - especially compared to my previous <a href="https://desmondchoy.netlify.app/2020/04/food-consumption-and-co2-emissions/">tidymodels post</a>.</p>
<pre class="r"><code>lr_wflow &lt;- workflow() %&gt;% 
  add_model(lr_model) %&gt;% 
  add_recipe(hiphop_rec)
  
lr_wflow</code></pre>
<pre><code>## == Workflow ===============================================================================================
## Preprocessor: Recipe
## Model: logistic_reg()
## 
## -- Preprocessor -------------------------------------------------------------------------------------------
## 6 Recipe Steps
## 
## * step_tokenize()
## * step_stopwords()
## * step_tokenfilter()
## * step_tfidf()
## * step_normalize()
## * step_dummy()
## 
## -- Model --------------------------------------------------------------------------------------------------
## Logistic Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = tune()
##   mixture = 1
## 
## Computational engine: glmnet</code></pre>
</div>
<div id="tuning" class="section level2">
<h2>Tuning</h2>
<p>Recall the penalty hyperparamter in our model was given the placeholder <code>tune()</code>. To find an optimal value, we first create a set of possible regularization parameters to conduct a grid search.</p>
<pre class="r"><code>grid &lt;- grid_regular(penalty(), levels = 40)</code></pre>
<p>Together with our resampled tibble created earlier, we can begin tuning. We can further specify relevant metrics - in this case, <code>roc_auc</code>, positive predictive value (<code>ppv</code>), negative predictive value (<code>npv</code>), and mean log loss (<code>mn_log_loss</code>) which takes into account the uncertainty in the prediction and gives a more detailed view into the actual performance.</p>
<pre class="r"><code>#doParallel::registerDoParallel()
#Step recommended by Julia Silge. Gives me an error on my Windows PC but perhaps it works on Mac?

set.seed(2000)
lr_grid &lt;- tune_grid(
  lr_wflow,
  resamples = folds,
  grid = grid,
  metrics = metric_set(roc_auc, npv, ppv, mn_log_loss))</code></pre>
<p>With our tuning results, we use <code>collect_metrics()</code> and visualize our findings.</p>
<pre class="r"><code>lr_grid %&gt;%
  collect_metrics() %&gt;%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line(size = 1.5, show.legend = FALSE) +
  facet_wrap(. ~ .metric, nrow = 4) +
  scale_x_log10(label = scales::number_format())</code></pre>
<p><img src="/post/2020-05-16-hip-hop-lyrics-be-flowin-with-tidy-text-mining_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>It does seem the optimal penalty value is below 0.01 - a higher mean is better for npv, ppv and roc_auc, while a lower mean is better for the log_loss metric. Beyond that, the trade-off is ppv gets better while the other three metrics suffer.</p>
<p>Using <code>show_best()</code>, we can see the optimal penalty values of roc_auc and mn_log_loss.</p>
<pre class="r"><code>lr_grid %&gt;%
  show_best(&quot;mn_log_loss&quot;) %&gt;%
  slice(1:3) %&gt;%
  bind_rows(lr_grid %&gt;%
              show_best(&quot;roc_auc&quot;) %&gt;%
              slice(1:3))</code></pre>
<pre><code>## # A tibble: 6 x 7
##   penalty .metric     .estimator  mean     n std_err .config
##     &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;fct&gt;  
## 1 0.00492 mn_log_loss binary     0.565    10 0.00506 Model31
## 2 0.00273 mn_log_loss binary     0.567    10 0.00741 Model30
## 3 0.00889 mn_log_loss binary     0.599    10 0.00282 Model32
## 4 0.00273 roc_auc     binary     0.785    10 0.00554 Model30
## 5 0.00492 roc_auc     binary     0.784    10 0.00571 Model31
## 6 0.00151 roc_auc     binary     0.781    10 0.00550 Model29</code></pre>
<p>I will go with the penalty value that maximises roc_auc, since it corresponds to the penalty value associated with the second lowest log_loss mean. Updating the workflow with our tuned hyperparamter value is done using <code>finalize_workflow()</code>.</p>
<pre class="r"><code>best_auc &lt;- lr_grid %&gt;%
  select_best(&quot;roc_auc&quot;)

final_lr &lt;- lr_wflow %&gt;%
  finalize_workflow(best_auc)
final_lr</code></pre>
<pre><code>## == Workflow ===============================================================================================
## Preprocessor: Recipe
## Model: logistic_reg()
## 
## -- Preprocessor -------------------------------------------------------------------------------------------
## 6 Recipe Steps
## 
## * step_tokenize()
## * step_stopwords()
## * step_tokenfilter()
## * step_tfidf()
## * step_normalize()
## * step_dummy()
## 
## -- Model --------------------------------------------------------------------------------------------------
## Logistic Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = 0.00272833337648676
##   mixture = 1
## 
## Computational engine: glmnet</code></pre>
</div>
<div id="fitting-visualizing-the-results" class="section level2">
<h2>Fitting / Visualizing The Results</h2>
<p>After fitting the model to the training data, let’s take a look at variable importance i.e. which words had the highest predictive power.</p>
<pre class="r"><code>library(vip)

labels &lt;- c(`NEG` = &quot;Modern&quot;,
            `POS` = &quot;Old School&quot;)

final_lr %&gt;%
  fit(hiphop_train) %&gt;%
  pull_workflow_fit() %&gt;%
  vi(lambda = best_auc$penalty) %&gt;%
  group_by(Sign) %&gt;%
  top_n(15, wt = abs(Importance)) %&gt;%
  ungroup() %&gt;%
  mutate(
    Importance = abs(Importance),
    Variable = str_remove(Variable, &quot;tfidf_lyric_&quot;),
    Variable = fct_reorder(Variable, Importance)
  ) %&gt;%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col(show.legend = FALSE) +
  facet_wrap( ~ Sign, scales = &quot;free&quot;, labeller = as_labeller(labels)) +
  labs(y = NULL,
       title = &quot;Hip-hop lyrics with the highest importance when determining the era of a song&quot;,
       subtitle = &quot;Modern: Released in 2000 or after; Old School: Songs released before 2000&quot;) +
  theme(
    plot.title = element_text(face = &quot;bold&quot;, size = 20),
    plot.title.position = &quot;plot&quot;,
    plot.subtitle = element_text(size = 15),
    strip.background = element_blank(),
    strip.text = element_text(face = &quot;bold&quot;, size = 15)
  )</code></pre>
<p><img src="/post/2020-05-16-hip-hop-lyrics-be-flowin-with-tidy-text-mining_files/figure-html/unnamed-chunk-23-1.png" width="1152" /></p>
<p><code>gender_mixed</code> immediately stands out as an outlier. Taking a look at lyrics data once more, it is now apparent why <code>gender_mixed</code> contributed improportionately. Out of all of the songs that were tagged as <code>gender_mixed</code>, lyrics were only pulled for one!</p>
<p>That means the entire dataset only had one datapoint tagged as <code>gender_mixed</code> - making it an unreliable predictor. As unfortunate as this, this serves as an excellent learning example for myself of the importance of data visualization and variable importance.</p>
<pre class="r"><code>lyrics %&gt;% 
  distinct(title, bucket, gender) %&gt;% 
  count(bucket, gender)</code></pre>
<pre><code>## # A tibble: 5 x 3
##   bucket     gender     n
##   &lt;chr&gt;      &lt;chr&gt;  &lt;int&gt;
## 1 Modern     female     8
## 2 Modern     male      61
## 3 Old School female     5
## 4 Old School male      67
## 5 Old School mixed      1</code></pre>
</div>
<div id="re-fitting-the-model" class="section level2">
<h2>Re-fitting the Model</h2>
<p>Because of this anomality in gender training data, I’ll remove gender as a predictor and re-run the entire process.</p>
<pre class="r"><code>#update recipe
hiphop_rec_v2 &lt;- hiphop_train %&gt;%
  recipe(bucket ~ lyric) %&gt;%
  step_tokenize(lyric) %&gt;%
  step_stopwords(lyric) %&gt;%
  step_tokenfilter(lyric, max_tokens = 2000) %&gt;%
  step_tfidf(lyric) %&gt;%
  step_normalize(all_predictors())

#update workflow
lr_wflow_v2 &lt;- lr_wflow %&gt;%
  update_recipe(hiphop_rec_v2)

#re-tune
set.seed(2000)
lr_grid_v2 &lt;- tune_grid(
  lr_wflow_v2,
  resamples = folds,
  grid = grid,
  metrics = metric_set(roc_auc, npv, ppv, mn_log_loss))

#visualization
lr_grid_v2 %&gt;% 
  collect_metrics() %&gt;% 
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line(size = 1.5, show.legend = FALSE) +
  facet_wrap(. ~ .metric, nrow = 4) +
  scale_x_log10(label = scales::number_format())</code></pre>
<p><img src="/post/2020-05-16-hip-hop-lyrics-be-flowin-with-tidy-text-mining_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Optimal penalty parameter of 0.0273 remains unchanged.</p>
<pre class="r"><code>lr_grid_v2 %&gt;%
  show_best(&quot;mn_log_loss&quot;) %&gt;%
  slice(1:3) %&gt;% 
  bind_rows(lr_grid_v2 %&gt;%
              show_best(&quot;roc_auc&quot;) %&gt;%
              slice(1:3))</code></pre>
<pre><code>## # A tibble: 6 x 7
##   penalty .metric     .estimator  mean     n std_err .config
##     &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;fct&gt;  
## 1 0.00492 mn_log_loss binary     0.570    10 0.00563 Model31
## 2 0.00273 mn_log_loss binary     0.573    10 0.00807 Model30
## 3 0.00889 mn_log_loss binary     0.604    10 0.00310 Model32
## 4 0.00273 roc_auc     binary     0.780    10 0.00582 Model30
## 5 0.00492 roc_auc     binary     0.777    10 0.00605 Model31
## 6 0.00151 roc_auc     binary     0.776    10 0.00569 Model29</code></pre>
<pre class="r"><code>best_auc_v2 &lt;- lr_grid_v2 %&gt;% 
  select_best(&quot;roc_auc&quot;)

final_lr_v2 &lt;- lr_wflow_v2 %&gt;% 
  finalize_workflow(best_auc_v2)</code></pre>
</div>
<div id="visualizing-results-once-more" class="section level2">
<h2>Visualizing Results Once More</h2>
<p>After filtering for stop words, we used tf-idf as one of our recipe steps to extract importance. The <a href="https://www.tidytextmining.com/tfidf.html#the-bind_tf_idf-function">idea behind tf-idf</a> is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection of documents. <strong>Calculating tf-idf attempts to find the words that are important (i.e. common) in a text, but not too common</strong>.</p>
<pre class="r"><code>final_lr_v2 %&gt;%
  fit(hiphop_train) %&gt;%
  pull_workflow_fit() %&gt;%
  vi(lambda = best_auc$penalty) %&gt;%
  group_by(Sign) %&gt;%
  top_n(15, wt = abs(Importance)) %&gt;%
  ungroup() %&gt;%
  mutate(
    Importance = abs(Importance),
    Variable = str_remove(Variable, &quot;tfidf_lyric_&quot;),
    Variable = fct_reorder(Variable, Importance)
  ) %&gt;%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ Sign, scales = &quot;free&quot;, labeller = as_labeller(labels)) +
  labs(y = NULL,
       title = &quot;Hip-hop lyrics with the highest importance when determining the era of a song&quot;,
       subtitle = &quot;Modern: Released in 2000 or after; Old School: Songs released before 2000&quot;) +
  theme(
    plot.title = element_text(face = &quot;bold&quot;, size = 20),
    plot.title.position = &quot;plot&quot;,
    plot.subtitle = element_text(size = 15),
    strip.background = element_blank(),
    strip.text = element_text(face = &quot;bold&quot;, size = 15)
  )</code></pre>
<p><img src="/post/2020-05-16-hip-hop-lyrics-be-flowin-with-tidy-text-mining_files/figure-html/unnamed-chunk-27-1.png" width="1152" /></p>
</div>
<div id="final-fit-on-test-data" class="section level2">
<h2>Final fit on Test Data</h2>
<p>Now we perform the final fit using <code>last_fit()</code> to evaluate our model on the test data. Somewhat counter-intuitively, <code>last_fit()</code> is fitted onto our initial split data instead of test data.</p>
<pre class="r"><code>hiphop_final &lt;- final_lr_v2 %&gt;% 
  last_fit(hiphop_split)

hiphop_final %&gt;% 
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.707
## 2 roc_auc  binary         0.778</code></pre>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>How did our model do? Our LASSO model, post tuning, gave a roc_auc mean of 0.78, while performance on test data was 0.778 - indicating we did not overfit. While I wouldn’t say performance is excellent, the model did perform better than random chance (46%/54% chance to pick Modern/Old School). Some considerations which could increase performance: Trying other models, increasing the number of tokens, and increasing the sample size of songs.</p>
<pre class="r"><code>lyrics %&gt;% 
  count(bucket) %&gt;% 
  mutate(percent = n/sum(n))</code></pre>
<pre><code>## # A tibble: 2 x 3
##   bucket         n percent
##   &lt;chr&gt;      &lt;int&gt;   &lt;dbl&gt;
## 1 Modern      5447   0.465
## 2 Old School  6278   0.535</code></pre>
<p>Writing and re-iterating through this blog post was fascinating and educational - giving me preliminary insight to the domain of NLP and another opportunity to familiarise myself with <code>tidymodels</code>. Lastly, the tidy text mining workflow is testament to how robust the <code>dplyr</code> package is when it comes to data wrangling.</p>
</div>
